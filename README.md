# Integration Automation

Automates the propagation of **endee** Python vector database client changes to downstream framework integrations â€” CrewAI, LangChain, and LlamaIndex â€” using a four-stage AI pipeline triggered directly from Slack.

---

## How It Works

When you publish a new version of the **endee** Python client, this system:

1. **Compares** the old and new client versions and generates a diff report
2. **Analyses** the diff with Claude to extract structured changes
3. **Updates** all three integration libraries in parallel (Claude writes the code)
4. **Tests** each integration and auto-heals failures (Claude fixes and retries up to 3Ã—)
5. **Opens Pull Requests** on GitHub for every passing integration
6. **Notifies** you in Slack with test results and PR links

Everything is triggered from a single Slack message or slash command â€” no manual intervention required.

---

## Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              SLACK BOT                        â”‚
â”‚                                              â”‚
â”‚  run 0.1.9 0.1.13          â† generate diff  â”‚
â”‚  automate                  â† trigger pipelineâ”‚
â”‚  /sync-clients             â† slash command   â”‚
â”‚    --branch feature/x                        â”‚
â”‚    --scope all                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚          FASTAPI SERVER  (main.py)            â”‚
â”‚                                              â”‚
â”‚  POST /slack/events                          â”‚
â”‚  POST /slack/slash/sync-clients              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚  background thread
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚        PYTHON ORCHESTRATOR  (pipeline.py)     â”‚
â”‚                                              â”‚
â”‚  Stage 1 â”€â”€ Analyze Diff                     â”‚
â”‚    claude -p "parse report â†’ JSON"           â”‚
â”‚                                              â”‚
â”‚  Stage 2 â”€â”€ Transform  (Ã—3 parallel)         â”‚
â”‚    claude -p "update crewai codeâ€¦"    â”      â”‚
â”‚    claude -p "update langchain codeâ€¦" â”œâ”€â”€    â”‚
â”‚    claude -p "update llamaindex codeâ€¦"â”˜      â”‚
â”‚                                              â”‚
â”‚  Stage 3 â”€â”€ Test + Self-Heal                 â”‚
â”‚    pytest â†’ fail â†’ claude -p "fix it" â†’ retryâ”‚
â”‚                                              â”‚
â”‚  Stage 4 â”€â”€ Create Pull Requests             â”‚
â”‚    claude -p "write PR title + bodyâ€¦"        â”‚
â”‚    gh pr create â€¦                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Project Structure

```
integration_automation/
â”‚
â”œâ”€â”€ main.py                          # FastAPI app â€” Slack event + slash command handler
â”‚
â”œâ”€â”€ orchestrator/
â”‚   â”œâ”€â”€ pipeline.py                  # Main coordinator â€” chains all 4 stages
â”‚   â”œâ”€â”€ stages/
â”‚   â”‚   â”œâ”€â”€ analyze.py               # Stage 1: claude -p to parse comparison report
â”‚   â”‚   â”œâ”€â”€ transform.py             # Stage 2: claude -p Ã—3 (parallel) to update code
â”‚   â”‚   â”œâ”€â”€ test_heal.py             # Stage 3: pytest + claude -p self-heal loop
â”‚   â”‚   â””â”€â”€ pull_request.py          # Stage 4: claude -p for PR content + gh pr create
â”‚   â””â”€â”€ utils/
â”‚       â”œâ”€â”€ claude_cli.py            # subprocess wrapper for `claude -p`
â”‚       â””â”€â”€ git_utils.py             # git clone / pull / commit / push helpers
â”‚
â”œâ”€â”€ integrations/
â”‚   â”œâ”€â”€ crewai_endee/tools.py        # CrewAI BaseTool wrapping endee Index.query()
â”‚   â”œâ”€â”€ langchain_endee/vectorstore.py  # LangChain VectorStore wrapping endee
â”‚   â””â”€â”€ llamaindex_endee/vector_store.py  # LlamaIndex VectorStore wrapping endee
â”‚
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_crewai_integration.py   # 12 tests â€” all mock-based, no endee required
â”‚   â”œâ”€â”€ test_langchain_integration.py  # 12 tests
â”‚   â””â”€â”€ test_llamaindex_integration.py # 13 tests
â”‚
â”œâ”€â”€ comparison_report.txt            # Generated by `run v1 v2` â€” consumed by pipeline
â”œâ”€â”€ requirements.txt
â””â”€â”€ .env.example
```

---

## Prerequisites

| Tool | Purpose | Install |
|------|---------|---------|
| Python 3.11+ | Runtime | [python.org](https://python.org) |
| Claude Code CLI | Powers every pipeline stage | `npm install -g @anthropic-ai/claude-code` |
| GitHub CLI (`gh`) | Opens pull requests | `brew install gh` |
| Git | Branch / commit / push | pre-installed on most systems |

---

## Setup

### 1. Clone and install

```bash
git clone https://github.com/your-org/integration_automation
cd integration_automation
python -m venv venv && source venv/bin/activate
pip install -r requirements.txt
```

### 2. Authenticate the Claude Code CLI

```bash
claude login
# Follow the browser prompt to authenticate with your Anthropic account
```

### 3. Configure environment variables

```bash
cp .env.example .env
# Edit .env and fill in every value
```

| Variable | Description |
|----------|-------------|
| `SLACK_BOT_TOKEN` | `xoxb-â€¦` bot token from your Slack app |
| `SLACK_APP_TOKEN` | `xapp-â€¦` app-level token (for socket mode if used) |
| `SLACK_SIGNING_SECRET` | Used to verify Slack request signatures |
| `ANTHROPIC_API_KEY` | Anthropic API key (also used by the Claude CLI) |
| `GITHUB_TOKEN` | Personal access token with `repo` scope |
| `CREWAI_REPO` | GitHub repo in `owner/repo` format |
| `LANGCHAIN_REPO` | GitHub repo in `owner/repo` format |
| `LLAMAINDEX_REPO` | GitHub repo in `owner/repo` format |
| `CREWAI_REPO_PATH` | Local path to cloned CrewAI integration repo |
| `LANGCHAIN_REPO_PATH` | Local path to cloned LangChain integration repo |
| `LLAMAINDEX_REPO_PATH` | Local path to cloned LlamaIndex integration repo |
| `COMPARISON_REPORT_PATH` | Path to the diff report file (default: `comparison_report.txt`) |

### 4. Configure Slack

In your [Slack app settings](https://api.slack.com/apps):

- **Event Subscriptions** â†’ Request URL: `https://your-host/slack/events`
  - Subscribe to bot events: `message.channels`, `message.im`
- **Slash Commands** â†’ Create `/sync-clients`
  - Request URL: `https://your-host/slack/slash/sync-clients`
- **OAuth Scopes** â†’ Bot Token Scopes: `chat:write`, `commands`

### 5. Run the server

```bash
uvicorn main:app --host 0.0.0.0 --port 8000 --reload
```

---

## Usage

### Step 1 â€” Generate the comparison report

Send a message in any Slack channel where the bot is present:

```
run 0.1.9 0.1.13
```

The bot downloads both versions from PyPI, diffs them, and saves `comparison_report.txt`. It posts the results back to Slack.

### Step 2 â€” Run the automation pipeline

**Option A â€” Slash command (recommended, supports flags):**

```
/sync-clients
/sync-clients --branch feature/endee-v0.1.13 --scope all
/sync-clients --branch hotfix/crewai-only    --scope crewai
/sync-clients --branch feature/x             --scope crewai,langchain
```

**Option B â€” Message trigger:**

```
automate
automate integrations
```

### What happens next

The bot immediately acknowledges your command and posts a threaded summary as each stage completes:

```
ğŸ¤– @you triggered sync-clients (branch: `feature/endee-v0.1.13`, scope: `all`)
  ğŸ” Stage 1/4 â€” Analysing diff â€¦
  âœ… Analysis complete â€” 6 change(s), 2 new parameter(s) detected.
  âš¡ Stage 2/4 â€” Updating integration code (3 clients in parallel) â€¦
  âœ… Transform complete â€” 3/3 succeeded.
  ğŸ§ª Stage 3/4 â€” Running tests (auto-healing on failure) â€¦
  âœ… crewai    â€” 12 passed (first run)
  âœ… langchain â€” 12 passed (first run)
  âœ… llamaindex â€” 13 passed (2 rounds)
  ğŸš€ Stage 4/4 â€” Creating GitHub Pull Requests â€¦
  âœ… crewai    PR â†’ https://github.com/your-org/crewai-endee/pull/42
  âœ… langchain PR â†’ https://github.com/your-org/langchain-endee/pull/17
  âœ… llamaindex PR â†’ https://github.com/your-org/llamaindex-endee/pull/9
  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  âœ… Pipeline finished successfully!
```

---

## Running Tests Locally

```bash
source venv/bin/activate
pytest tests/ -v
```

All 37 tests run with mocks â€” no endee client, no Slack, no GitHub credentials required.

```
tests/test_crewai_integration.py    12 passed
tests/test_langchain_integration.py 12 passed
tests/test_llamaindex_integration.py 13 passed
```

---

## Integrations

Each integration wraps `endee.Index.query()` for its respective framework and exposes the new parameters introduced in endee v0.1.13:

| Parameter | Type | Default | Range | Description |
|-----------|------|---------|-------|-------------|
| `prefilter_cardinality_threshold` | `int` | `10000` | 1,000 â€“ 1,000,000 | Switches from HNSW filtered search to brute-force prefiltering below this cardinality |
| `filter_boost_percentage` | `int` | `0` | 0 â€“ 100 | Expands the HNSW candidate pool by this percentage when a filter is active |

### CrewAI

```python
from endee import Index
from integrations.crewai_endee import EndeeSearchTool

index = Index(name="my-index", url="https://...", api_key="...")
tool  = EndeeSearchTool(index=index)

# Use in a CrewAI agent
results = tool._run(
    query_vector=[0.1, 0.2, 0.3],
    top_k=10,
    filter=[{"category": {"$eq": "A"}}],
    prefilter_cardinality_threshold=5_000,
    filter_boost_percentage=20,
)
```

### LangChain

```python
from endee import Index
from integrations.langchain_endee import EndeeVectorStore

index       = Index(name="my-index", url="https://...", api_key="...")
embed_fn    = lambda text: my_embedding_model.embed(text)
vector_store = EndeeVectorStore(index=index, embedding_function=embed_fn)

docs = vector_store.similarity_search(
    "What is RAG?",
    k=5,
    filter=[{"category": {"$eq": "A"}}],
    prefilter_cardinality_threshold=5_000,
    filter_boost_percentage=20,
)
```

### LlamaIndex

```python
from endee import Index
from integrations.llamaindex_endee import EndeeVectorStore

index        = Index(name="my-index", url="https://...", api_key="...")
vector_store = EndeeVectorStore(index=index)

# Pass new params via query_kwargs
from llama_index.core.vector_stores.types import VectorStoreQuery
result = vector_store.query(
    VectorStoreQuery(query_embedding=[0.1, 0.2, 0.3], similarity_top_k=5),
    prefilter_cardinality_threshold=5_000,
    filter_boost_percentage=20,
)
```

---

## How the Pipeline Uses Claude

Each stage calls `claude -p "<prompt>"` as a subprocess â€” no LLM SDK is imported. The Claude Code CLI handles authentication and streaming internally.

| Stage | Claude's job |
|-------|-------------|
| Analyze | Parse the raw diff â†’ structured JSON of changes and new parameters |
| Transform | Read current integration file + changes â†’ return the complete updated file |
| Self-Heal | Read pytest failure output + broken code â†’ return the corrected file |
| PR content | Summarise changes â†’ return PR title and markdown body |

---

## Scope Flag Reference

| `--scope` value | Integrations updated |
|-----------------|----------------------|
| `all` (default) | crewai, langchain, llamaindex |
| `crewai` | CrewAI only |
| `langchain` | LangChain only |
| `llamaindex` | LlamaIndex only |
| `crewai,langchain` | CrewAI + LangChain |
